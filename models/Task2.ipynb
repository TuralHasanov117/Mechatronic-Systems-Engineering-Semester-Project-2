{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**"
      ],
      "metadata": {
        "id": "0cm4Xu6fL6Dj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yr2uxbZmKqQR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Cuda Availability"
      ],
      "metadata": {
        "id": "5bAVqEDJLwpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "-BYdL8L5LwFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8519bee-5710-4c71-e49c-9f512e37f40d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount google drive"
      ],
      "metadata": {
        "id": "xn_JbiEcL2B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5p2SZm6SL3hY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761eed8f-5b55-4248-a253-3398728c089a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define data path"
      ],
      "metadata": {
        "id": "9itZJoP9MDlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_priority_path = '/content/drive/My Drive/data/Priority'\n",
        "class_stop_path = '/content/drive/My Drive/data/Stop'"
      ],
      "metadata": {
        "id": "lCaB6lJ2MEbk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priority_images = [image for image in os.listdir(class_priority_path) if image.endswith('.jpg')]\n",
        "stop_images = [image for image in os.listdir(class_stop_path) if image.endswith('.jpg')]\n",
        "labeled_priority_images_full_path = [(os.path.join(class_priority_path, image), 1) for image in priority_images]\n",
        "labeled_stop_images_full_path = [(os.path.join(class_stop_path, image), 0) for image in stop_images]\n",
        "data = labeled_priority_images_full_path + labeled_stop_images_full_path"
      ],
      "metadata": {
        "id": "OJ4iCWMqSK8N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data to x and y (features and label)"
      ],
      "metadata": {
        "id": "bWBD7JnOMLgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_array = []\n",
        "y_array = []\n",
        "\n",
        "for item in data:\n",
        "  x_image = cv2.imread(item[0])\n",
        "  x_image = cv2.resize(x_image, (224, 224))\n",
        "  X_array.append(x_image)\n",
        "  y_image = item[1]\n",
        "  y_array.append(y_image)\n",
        "\n",
        "X_array = np.array(X_array)\n",
        "y_array = np.array(y_array)"
      ],
      "metadata": {
        "id": "Uh6WS55eMe4S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Data to Tensor"
      ],
      "metadata": {
        "id": "pF35P4kWWOCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X_array, dtype=torch.float32)\n",
        "y = torch.tensor(y_array, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "LhlG-aQ5WHrW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test Split"
      ],
      "metadata": {
        "id": "Pp-aJMIeUSmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=122)\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_test = y_test.to(device)"
      ],
      "metadata": {
        "id": "4PPbfYcqUPLb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load ResNet18"
      ],
      "metadata": {
        "id": "UXvI_qfKUlNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseModel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "baseModel.fc = nn.Linear(512, 512)"
      ],
      "metadata": {
        "id": "aNpSQfGmUnOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b0df17-85fb-4bba-8944-8541dd6086e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 90.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Model"
      ],
      "metadata": {
        "id": "u9tKeaoIWYak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_units = 128\n",
        "output_units = 2\n",
        "activation_function = nn.Sigmoid()"
      ],
      "metadata": {
        "id": "WoNIdUtoWWw4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headModel = nn.AdaptiveAvgPool2d((1,1))\n",
        "hidden_layer = nn.Linear(512, hidden_units)\n",
        "dropout_layer = nn.Dropout(p=0.5)\n",
        "output_layer = nn.Linear(hidden_units, output_units)\n",
        "model_resnet18_classification = nn.Sequential(baseModel, hidden_layer, activation_function, output_layer)\n",
        "model_resnet18_classification.to(device)"
      ],
      "metadata": {
        "id": "jl6ACZPNWWIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0120ce27-436e-43b4-9fb0-dac6165346bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (1): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (2): Sigmoid()\n",
              "  (3): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Data"
      ],
      "metadata": {
        "id": "FNflwO5eWjbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,train_size=0.9, test_size=0.1, random_state=42)\n",
        "\n",
        "epochs = 30\n",
        "batch_size = 16\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "num_batches_train = (len(X_train) + batch_size - 1) // batch_size\n",
        "num_batches_val = (len(X_val) + batch_size - 1) // batch_size\n",
        "\n",
        "optimizer = optim.Adam(model_resnet18_classification.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  model_resnet18_classification.train().to(device)\n",
        "  train_loss = 0\n",
        "\n",
        "  for i in range(num_batches_train):\n",
        "    batch_inputs = X_train[i * batch_size:(i + 1) * batch_size].permute(0, 3, 1, 2) # (N, C, H, W, C, N)\n",
        "    batch_labels = y_train[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_resnet18_classification(batch_inputs)\n",
        "    loss = loss_function(outputs, batch_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  epoch_loss = train_loss / num_batches_train\n",
        "\n",
        "  model_resnet18_classification.eval().to(device)\n",
        "  val_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for j in range(num_batches_val):\n",
        "      batch_inputs = X_val[j * batch_size:(j + 1) * batch_size].permute(0, 3, 1, 2)\n",
        "      batch_labels = y_val[j * batch_size:(j + 1) * batch_size]\n",
        "\n",
        "      outputs = model_resnet18_classification(batch_inputs)\n",
        "      loss = loss_function(outputs, batch_labels)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    val_loss /= num_batches_val\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "  epoch_train_loss = train_loss / num_batches_train\n",
        "\n",
        "  print(f'epoch {epoch+1} Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')"
      ],
      "metadata": {
        "id": "lIABGCbLWkpz",
        "outputId": "495d2b29-0b69-46ee-d8c2-c7d577d40686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 Train Loss: 5.779869, Val Loss: 0.044204\n",
            "epoch 2 Train Loss: 2.251409, Val Loss: 0.241062\n",
            "epoch 3 Train Loss: 1.874122, Val Loss: 0.670971\n",
            "epoch 4 Train Loss: 0.471400, Val Loss: 0.014633\n",
            "epoch 5 Train Loss: 0.207992, Val Loss: 0.008807\n",
            "epoch 6 Train Loss: 0.141926, Val Loss: 0.006544\n",
            "epoch 7 Train Loss: 0.108880, Val Loss: 0.005261\n",
            "epoch 8 Train Loss: 0.088598, Val Loss: 0.004376\n",
            "epoch 9 Train Loss: 0.074289, Val Loss: 0.003722\n",
            "epoch 10 Train Loss: 0.063569, Val Loss: 0.003218\n",
            "epoch 11 Train Loss: 0.055239, Val Loss: 0.002820\n",
            "epoch 12 Train Loss: 0.048594, Val Loss: 0.002497\n",
            "epoch 13 Train Loss: 0.043184, Val Loss: 0.002232\n",
            "epoch 14 Train Loss: 0.038706, Val Loss: 0.002010\n",
            "epoch 15 Train Loss: 0.034948, Val Loss: 0.001822\n",
            "epoch 16 Train Loss: 0.031755, Val Loss: 0.001662\n",
            "epoch 17 Train Loss: 0.029016, Val Loss: 0.001523\n",
            "epoch 18 Train Loss: 0.026644, Val Loss: 0.001402\n",
            "epoch 19 Train Loss: 0.024573, Val Loss: 0.001297\n",
            "epoch 20 Train Loss: 0.022753, Val Loss: 0.001203\n",
            "epoch 21 Train Loss: 0.021142, Val Loss: 0.001120\n",
            "epoch 22 Train Loss: 0.019709, Val Loss: 0.001046\n",
            "epoch 23 Train Loss: 0.018428, Val Loss: 0.000980\n",
            "epoch 24 Train Loss: 0.017276, Val Loss: 0.000920\n",
            "epoch 25 Train Loss: 0.016236, Val Loss: 0.000866\n",
            "epoch 26 Train Loss: 0.015293, Val Loss: 0.000817\n",
            "epoch 27 Train Loss: 0.014436, Val Loss: 0.000772\n",
            "epoch 28 Train Loss: 0.013653, Val Loss: 0.000731\n",
            "epoch 29 Train Loss: 0.012936, Val Loss: 0.000693\n",
            "epoch 30 Train Loss: 0.012278, Val Loss: 0.000659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model"
      ],
      "metadata": {
        "id": "jEcyg134W7ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_resnet18_classification.state_dict(), 'model_resnet18_classification.pth')"
      ],
      "metadata": {
        "id": "Oi9yBOVKW0Sl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model"
      ],
      "metadata": {
        "id": "JnMFBRE3W5iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseModel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "baseModel.fc = torch.nn.Linear(512, 512)\n",
        "headModel = torch.nn.AdaptiveAvgPool2d((1,1))\n",
        "hidden_layer = torch.nn.Linear(512, 128)\n",
        "dropout_layer = torch.nn.Dropout(p=0.5)\n",
        "output_layer = torch.nn.Linear(128, 2)\n",
        "model_resnet18_classification = torch.nn.Sequential(baseModel, hidden_layer, torch.nn.Sigmoid(), output_layer)\n",
        "model_resnet18_classification = model_resnet18_classification.to(device).eval()\n",
        "model_resnet18_classification.load_state_dict(torch.load('model_resnet18_classification.pth'))"
      ],
      "metadata": {
        "id": "BE1N_wHAW5A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80fce7cf-48f9-44ac-fd94-ab2c4d341a38"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "ZQag7C5RWpfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet18_classification.eval().to(device)\n",
        "\n",
        "num_batches_test = (len(X_test) + batch_size - 1) // batch_size\n",
        "actuals = []\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(num_batches_test):\n",
        "    batch_inputs_test = X_test[i * batch_size:(i + 1) * batch_size].permute(0, 3, 1, 2)\n",
        "    batch_labels_test = y_test[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "    outputs = model_resnet18_classification(batch_inputs_test)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "    actuals.append(batch_labels_test.cpu().numpy())\n",
        "    predictions.append(outputs.cpu().numpy())\n",
        "\n",
        "actuals = np.concatenate(actuals, axis=0)\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "accuracy = accuracy_score(actuals, predictions)\n",
        "precision = precision_score(actuals, predictions, average='weighted')\n",
        "recall = recall_score(actuals, predictions, average='weighted')\n",
        "f1 = f1_score(actuals, predictions, average='weighted')\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 Score:', f1)"
      ],
      "metadata": {
        "id": "K3QFv4PYWu85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "e2cff6f6-2122-43bd-a460-a29017946e16"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of binary and continuous-multioutput targets",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-681abfdb751c>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous-multioutput targets"
          ]
        }
      ]
    }
  ]
}